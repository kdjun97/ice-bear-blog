---
layout: post
title:  "PoseNet With Flutter"
date:   2021-06-27 14:40:30 +0800
categories: [Tech]
excerpt: PoseNet
tags:
  - Flutter
  - TensorFlow Lite
  - Machine Learning
---

# PoseNet

**PoseNet**은 주요 신체 관절의 위치를 예측하여 이미지 또는 비디오에서 사람의 포즈를 예측하는 데 사용할 수 있는 비전 모델이다.  
출처 : (https://www.tensorflow.org/lite/examples/pose_estimation/overview?hl=ko)  

코, 왼쪽 눈, 오른쪽 눈, 왼쪽 무릎, 오른쪽 무릎, 왼쪽 팔꿈치, 오른쪽 팔꿈치 등 신체의 관절 부위에 대한 point를 인식하여 그 point들을 선으로 그어 pose를 예측해준다.  

예제 역시 Tensorflow 홈페이지에서 확인이 가능하다.  
출처 : (https://www.tensorflow.org/lite/examples/pose_estimation/overview?hl=ko)  

### 수행 방법 ?

[Tensorflow](https://www.tensorflow.org/lite/examples/pose_estimation/overview?hl=ko) 홈페이지를 참고하였고, 이해하기 쉽게 그림을 첨부한다.  

![output](/assets/images/posenet/flow.PNG)  


### 구현

기존 tflite 모델을 flutter에 적용해보면, Accruacy가 상당히 낮고, 전처리가 부족하여 기대하는 output이 나오지 않는다는 것을 알 수 있었다.  
또한, 기존 모델의 예제는 그림이 아닌 비디오로 진행이 되었기 때문에, 이미지를 input으로 받았을 때, output으로 posenet이 적용된 화면을 띄워주는 예제를 만들어 보았다.  

pubspec.yaml  
```
dependencies : tflite: ^1.1.2   
assets:
    - assets/posenet_mv1_075_float_from_checkpoints.tflite  
```  

1. 기존 모델의 함수로 point를 추출한다.  
1. 기존 모델이 accuracy가 낮기 때문에, 전처리를 해주어야 했음.  
    1. point들마다 score(accuracy)가 50% 이상인 것들만 취급을 해주고,  
    2. 동일한 part(ex: 왼쪽 무릎)에 2개 이상의 point가 있을 경우, accruacy가 높은 경우만 살아남도록 해주고,  
    3. 중복된 포인트가 있다면 없애준다.  
1. 이정도로도 model의 performance는 훌륭해졌다.  
1. 이제 뽑은 point들에 대해 line을 그려주고,  
1. 모든 위젯들을 stack으로 보여주면 posenet이 완성된다.  

### 결과 

![output1](/assets/images/posenet/posenet_res1.png)  

